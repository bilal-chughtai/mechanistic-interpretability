{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/neelnanda-io/Easy-Transformer.git\n",
      "  Cloning https://github.com/neelnanda-io/Easy-Transformer.git to /tmp/pip-req-build-_jftmgft\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/Easy-Transformer.git /tmp/pip-req-build-_jftmgft\n",
      "  Resolved https://github.com/neelnanda-io/Easy-Transformer.git to commit fdd4fa591bc10b428c1154ea16d8577a9f909407\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (0.5.0)\n",
      "Requirement already satisfied: numpy in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (1.23.4)\n",
      "Requirement already satisfied: torch in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (1.12.1)\n",
      "Requirement already satisfied: datasets in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: transformers in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (4.23.1)\n",
      "Requirement already satisfied: tqdm in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (4.64.1)\n",
      "Requirement already satisfied: pandas in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: wandb in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (0.13.4)\n",
      "Requirement already satisfied: fancy_einsum in /home/vscode/.local/lib/python3.9/site-packages (from easy-transformer==0.1.0) (0.0.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (0.3.5.1)\n",
      "Requirement already satisfied: aiohttp in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (2022.10.0)\n",
      "Requirement already satisfied: multiprocess in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (0.70.13)\n",
      "Requirement already satisfied: responses<0.19 in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (0.10.1)\n",
      "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (2.28.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/vscode/.local/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (9.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vscode/.local/lib/python3.9/site-packages (from pandas->easy-transformer==0.1.0) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/vscode/.local/lib/python3.9/site-packages (from pandas->easy-transformer==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /home/vscode/.local/lib/python3.9/site-packages (from torch->easy-transformer==0.1.0) (4.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/vscode/.local/lib/python3.9/site-packages (from transformers->easy-transformer==0.1.0) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vscode/.local/lib/python3.9/site-packages (from transformers->easy-transformer==0.1.0) (2022.9.13)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.9/site-packages (from transformers->easy-transformer==0.1.0) (3.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (58.1.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (4.21.9)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (3.1.29)\n",
      "Requirement already satisfied: setproctitle in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (5.9.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (1.10.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (8.1.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (1.0.9)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/vscode/.local/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (2.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (4.0.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/vscode/.local/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb->easy-transformer==0.1.0) (4.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/vscode/.local/lib/python3.9/site-packages (from packaging->datasets->easy-transformer==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vscode/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets->easy-transformer==0.1.0) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets->easy-transformer==0.1.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets->easy-transformer==0.1.0) (2022.9.24)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/vscode/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->easy-transformer==0.1.0) (5.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/neelnanda-io/Easy-Transformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# plotting\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = \"vscode\"\n",
    "\n",
    "# EasyTransformer interpretability tooling\n",
    "from easy_transformer.hook_points import HookedRootModule, HookPoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available:\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Training might be rather slow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting functions\n",
    "# This is mostly a bunch of over-engineered mess to hack Plotly into producing \n",
    "# the pretty pictures I want, I recommend not reading too closely unless you \n",
    "# want Plotly hacking practice\n",
    "def to_numpy(tensor, flat=False):\n",
    "    if type(tensor)!=torch.Tensor:\n",
    "        return tensor\n",
    "    if flat:\n",
    "        return tensor.flatten().detach().cpu().numpy()\n",
    "    else:\n",
    "        return tensor.detach().cpu().numpy()\n",
    "\n",
    "def imshow(tensor, xaxis=None, yaxis=None, animation_name='Snapshot', **kwargs):\n",
    "    # if tensor.shape[0]==p*p:\n",
    "    #     tensor = unflatten_first(tensor)\n",
    "    tensor = torch.squeeze(tensor)\n",
    "    px.imshow(to_numpy(tensor, flat=False), \n",
    "              labels={'x':xaxis, 'y':yaxis, 'animation_name':animation_name}, \n",
    "              **kwargs).show()\n",
    "\n",
    "# Set default colour scheme\n",
    "imshow_pos = partial(imshow, color_continuous_scale='Blues')\n",
    "# Creates good defaults for showing divergent colour scales (ie with both \n",
    "# positive and negative values, where 0 is white)\n",
    "\n",
    "imshow = partial(imshow, color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "# Presets a bunch of defaults to imshow to make it suitable for showing heatmaps \n",
    "# of activations with x axis being input 1 and y axis being input 2.\n",
    "\n",
    "inputs_heatmap = partial(imshow, xaxis='Input 1', yaxis='Input 2', color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "\n",
    "def line(x, y=None, hover=None, xaxis='', yaxis='', **kwargs):\n",
    "    if type(y)==torch.Tensor:\n",
    "        y = to_numpy(y, flat=True)\n",
    "    if type(x)==torch.Tensor:\n",
    "        x=to_numpy(x, flat=True)\n",
    "    fig = px.line(x, y=y, hover_name=hover, **kwargs)\n",
    "    fig.update_layout(xaxis_title=xaxis, yaxis_title=yaxis)\n",
    "    fig.show()\n",
    "\n",
    "def scatter(x, y, **kwargs):\n",
    "    px.scatter(x=to_numpy(x, flat=True), y=to_numpy(y, flat=True), **kwargs).show()\n",
    "\n",
    "def lines(lines_list, x=None, mode='lines', labels=None, xaxis='', yaxis='', title = '', log_y=False, hover=None, **kwargs):\n",
    "    # Helper function to plot multiple lines\n",
    "    if type(lines_list)==torch.Tensor:\n",
    "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
    "    if x is None:\n",
    "        x=np.arange(len(lines_list[0]))\n",
    "    fig = go.Figure(layout={'title':title})\n",
    "    fig.update_xaxes(title=xaxis)\n",
    "    fig.update_yaxes(title=yaxis)\n",
    "    for c, line in enumerate(lines_list):\n",
    "        if type(line)==torch.Tensor:\n",
    "            line = to_numpy(line)\n",
    "        if labels is not None:\n",
    "            label = labels[c]\n",
    "        else:\n",
    "            label = c\n",
    "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
    "    if log_y:\n",
    "        fig.update_layout(yaxis_type=\"log\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Group Parent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Group:\n",
    "    \"\"\"\n",
    "    parent class for all groups\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_multiplication_table():\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compose(self, x, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inverse(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Individual Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CyclicGroup(Group):\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        self.order = index\n",
    "        self.compute_multiplication_table()\n",
    "\n",
    "\n",
    "    def compose(self, x, y):\n",
    "        return (x+y)%self.order\n",
    "\n",
    "    def inverse(self, x):\n",
    "        return -x%self.order\n",
    "\n",
    "\n",
    "    def compute_multiplication_table(self):\n",
    "        table = torch.zeros((self.order, self.order), dtype=torch.int64)\n",
    "        for i in range(self.order):\n",
    "            for j in range(self.order):\n",
    "                table[i, j] = self.compose(i, j)\n",
    "        self.multiplication_table = table\n",
    "\n",
    "    def get_all_data(self, shuffle_seed = False):\n",
    "        data=torch.zeros((self.order*self.order, 3), dtype=torch.int64)\n",
    "        for i in range(self.order):\n",
    "            for j in range(self.order):\n",
    "                data[i*self.order+j, 0] = i\n",
    "                data[i*self.order+j, 1] = j\n",
    "                data[i*self.order+j, 2] = self.multiplication_table[i, j]\n",
    "        if shuffle_seed:\n",
    "            torch.manual_seed(shuffle_seed)\n",
    "            shuffled_indices = torch.randperm(self.order*self.order)\n",
    "            shuffled_arr = data[shuffled_indices]\n",
    "        return data\n",
    "        \n",
    "\n",
    "#class DihedralGroup(Group):\n",
    "\n",
    "\n",
    "#class SymettricGroup(Group):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearNet(HookedRootModule):\n",
    "    \"\"\"\n",
    "    A completely linear network. W1a and W1b are embedding layers, whose outputs are elementwise multiplied. The result is unembedded by W2.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, n, seed=0):\n",
    "        # hidden : hidden dimension size\n",
    "        # n : group order\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # initialise parameters\n",
    "        self.W1a = nn.Parameter(torch.randn(n, hidden)/np.sqrt(hidden))\n",
    "        self.W1b = nn.Parameter(torch.randn(n, hidden)/np.sqrt(hidden))\n",
    "        self.Wfinal = nn.Parameter(torch.randn(hidden, n)/np.sqrt(hidden))\n",
    "\n",
    "        self.x_embed = HookPoint()\n",
    "        self.y_embed = HookPoint()\n",
    "        self.product = HookPoint()\n",
    "        self.out = HookPoint()\n",
    "        \n",
    "        # We need to call the setup function of HookedRootModule to build an \n",
    "        # internal dictionary of modules and hooks, and to give each hook a name\n",
    "        super().setup()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data[:, 0] # (batch) \n",
    "        x_embed = self.x_embed(self.W1a[x]) # (batch, hidden)\n",
    "        y = data[:, 1]\n",
    "        y_embed = self.y_embed(self.W1b[y]) # (batch, hidden)\n",
    "        product = self.product(x_embed * y_embed) # (batch, hidden)\n",
    "        out = self.out(product @ self.Wfinal) #(batch, n)\n",
    "        return out\n",
    "\n",
    "class TwoLayerReLUNet(nn.Module):\n",
    "    def __init__(self, hiddens, n, seed=0):\n",
    "        # hidden : hidden dimension size\n",
    "        # n : group order\n",
    "        embed_dim, hidden = hiddens\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # xavier initialise parameters\n",
    "        self.W1a = nn.Parameter(torch.randn(n, embed_dim)/np.sqrt(embed_dim))\n",
    "        self.W1b = nn.Parameter(torch.randn(n, embed_dim)/np.sqrt(embed_dim))\n",
    "        self.W2 = nn.Parameter(torch.randn(2*embed_dim, hidden)/np.sqrt(2*embed_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.Wfinal = nn.Parameter(torch.randn(hidden, n)/np.sqrt(hidden))\n",
    "        self.hid = 0 #to access later\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data[:, 0] # (N, ) N is batch size\n",
    "        rho_x = self.W1a[x] # (N, embed_dim)\n",
    "        y = data[:, 1]\n",
    "        rho_y = self.W1b[y] # (N, embed_dim)\n",
    "        hid1 = torch.hstack((rho_x, rho_y)) # (N, 2*embed_dim)\n",
    "        self.hid = self.relu(hid1 @ self.W2) # (N, hidden)\n",
    "        return self.hid @ self.Wfinal #(N, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data and Loss Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_data(group, frac_train):\n",
    "    data = group.get_all_data().cuda()\n",
    "    train_size = int(frac_train*data.shape[0])\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    train_data = train[:, :2]\n",
    "    train_labels = train[:, 2]\n",
    "    test_data = test[:, :2]\n",
    "    test_labels = test[:, 2]\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m150000\u001b[39m\n\u001b[1;32m      7\u001b[0m group \u001b[39m=\u001b[39m CyclicGroup(\u001b[39m131\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_data, test_data, train_labels, test_labels \u001b[39m=\u001b[39m generate_train_test_data(group, frac_train)\n\u001b[1;32m     10\u001b[0m train_losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m test_losses \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m, in \u001b[0;36mgenerate_train_test_data\u001b[0;34m(group, frac_train)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_train_test_data\u001b[39m(group, frac_train):\n\u001b[0;32m----> 2\u001b[0m     data \u001b[39m=\u001b[39m group\u001b[39m.\u001b[39;49mget_all_data()\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m      3\u001b[0m     train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(frac_train\u001b[39m*\u001b[39mdata\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m     train \u001b[39m=\u001b[39m data[:train_size]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    218\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "frac_train = 0.3\n",
    "width = 256\n",
    "lr = 1e-3\n",
    "weight_decay = 0.3\n",
    "num_epochs = 150000\n",
    "\n",
    "group = CyclicGroup(131)\n",
    "train_data, test_data, train_labels, test_labels = generate_train_test_data(group, frac_train)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "model = BilinearNet(width, group.order)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_logits = model(train_data)\n",
    "    train_loss = loss_fn(train_logits, train_labels)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    train_losses.append(train_loss.item())\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(test_data)\n",
    "        test_loss = loss_fn(test_logits, test_labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "        train_acc = (train_logits.argmax(1)==train_labels).sum()/len(train_labels)\n",
    "        test_acc = (test_logits.argmax(1)==test_labels).sum()/len(test_labels)\n",
    "        train_accs.append(train_acc.item())\n",
    "        test_accs.append(test_acc.item())\n",
    "    if epoch%1000 == 0:\n",
    "        print(f\"Epoch:{epoch}\")\n",
    "        print(f\"Train: L: {train_losses[-1]:.6f} A: {train_accs[-1]*100:.4f}%\")\n",
    "        print(f\"Test: L: {test_losses[-1]:.6f} A: {test_accs[-1]*100:.4f}%\")\n",
    "    if epoch%10000 == 0 and epoch>0:\n",
    "        lines([train_losses, test_losses], log_y=True, labels=['train loss', 'test loss'])\n",
    "        lines([train_accs, test_accs], log_y=False, labels=['train acc', 'test acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability Set Up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at cosine similarity to representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over individual neurons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
